{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #FF6D6B\">\n",
    "\n",
    "#### Resources\n",
    "</span>\n",
    "\n",
    "- [Link to generate keys](https://aistudio.google.com/app/apikey)\n",
    "- [Gemini Cookbook](https://github.com/google-gemini/cookbook/tree/main)\n",
    "- [API Docs](https://ai.google.dev/gemini-api/docs/get-started/tutorial?lang=python)\n",
    "- [Prompt Engineering](https://www.promptingguide.ai/)\n",
    "- [OpenAI Cookbook for use-cases](https://cookbook.openai.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #FF6D6B\">\n",
    "\n",
    "#### Dependencies\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q -U google-generativeai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #FF6D6B\">\n",
    "\n",
    "#### Context\n",
    "</span>\n",
    "\n",
    "<span style=\"color: #805ec7\">\n",
    "\n",
    "\n",
    "Note: The retrieval_document task type is the only task that accepts a title.\n",
    "\n",
    "\n",
    "</span>\n",
    "\n",
    "| Task Type            | Description                                                                                      |\n",
    "|----------------------|--------------------------------------------------------------------------------------------------|\n",
    "| RETRIEVAL_QUERY      | Specifies the given text is a query in a search/retrieval setting.                               |\n",
    "| RETRIEVAL_DOCUMENT   | Specifies the given text is a document in a search/retrieval setting. Using this task type requires a title. |\n",
    "| SEMANTIC_SIMILARITY  | Specifies the given text will be used for Semantic Textual Similarity (STS).                     |\n",
    "| CLASSIFICATION       | Specifies that the embeddings will be used for classification.                                   |\n",
    "| CLUSTERING           | Specifies that the embeddings will be used for clustering.                                       |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #FF6D6B\">\n",
    "\n",
    "#### Load the model and select task type\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY=\"\"\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #FF6D6B\">\n",
    "\n",
    "#### Generate embeddings\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #30AADD\">\n",
    "\n",
    "##### Single string\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = genai.embed_content(\n",
    "    model=\"models/embedding-001\",\n",
    "    content=\"What is the meaning of life?\",\n",
    "    task_type=\"retrieval_document\",\n",
    "    title=\"Embedding of single string\")\n",
    "\n",
    "# 1 input > 1 vector output\n",
    "print(str(result['embedding'])[:50], '... TRIMMED]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #30AADD\">\n",
    "\n",
    "##### List of strings\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = genai.embed_content(\n",
    "    model=\"models/embedding-001\",\n",
    "    content=[\n",
    "      'What is the meaning of life?',\n",
    "      'How much wood would a woodchuck chuck?',\n",
    "      'How does the brain work?'],\n",
    "    task_type=\"retrieval_document\",\n",
    "    title=\"Embedding of list of strings\")\n",
    "\n",
    "# A list of inputs > A list of vectors output\n",
    "for v in result['embedding']:\n",
    "  print(str(v)[:50], '... TRIMMED ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = genai.embed_content(\n",
    "    model=\"models/embedding-001\",\n",
    "    content=\"What is the meaning of life?\",\n",
    "    task_type=\"retrieval_document\",\n",
    "    title=\"Embedding of single string\")\n",
    "\n",
    "# 1 input > 1 vector output\n",
    "print(str(result['embedding'])[:50], '... TRIMMED]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel('gemini-1.5-flash')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #FF6D6B\">\n",
    "\n",
    "#### Generate text from text inputs\n",
    "</span>\n",
    "\n",
    "For text-only prompts, use a Gemini 1.5 model or the Gemini 1.0 Pro model\n",
    "\n",
    "The generate_content method can handle a wide variety of use cases, including multi-turn chat and multimodal input, depending on what the underlying model supports. The available models only support text and images as input, and text as output.\n",
    "\n",
    "In the simplest case, you can pass a prompt string to the GenerativeModel.generate_content method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #30AADD\">\n",
    "\n",
    "##### Without streaming\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.generate_content(\"What is the meaning of life?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #30AADD\">\n",
    "\n",
    "##### With Streaming\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.generate_content(\"What is the meaning of life?\", stream=True)\n",
    "\n",
    "for chunk in response:\n",
    "  print(chunk.text)\n",
    "  print(\"_\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #FF6D6B\">\n",
    "\n",
    "#### Generate text from image and text inputs\n",
    "</span>\n",
    "\n",
    "<span style=\"color: #805ec7\">\n",
    "Note: If your prompts will exceed 20MB in size, upload the media files with the File API. [Link](https://ai.google.dev/tutorials/prompting_with_media)\n",
    "</span>\n",
    "\n",
    "Gemini provides various models that can handle multimodal input (Gemini 1.5 models and Gemini 1.0 Pro Vision) so that you can input both text and images. Image requirements for prompts. [Link](https://ai.google.dev/docs/\n",
    "gemini_api_overview#image_requirements)\n",
    "\n",
    "When the prompt input includes both text and images, use a Gemini 1.5 model or the Gemini 1.0 Pro Vision model with the GenerativeModel.generate_content method to generate text output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #30AADD\">\n",
    "\n",
    "##### Only image\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "\n",
    "img = PIL.Image.open('image.jpg')\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel('gemini-1.5-flash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.generate_content(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #30AADD\">\n",
    "\n",
    "##### Image + Text\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.generate_content([\"Write a short, engaging blog post based on this picture. It should include a description of the meal in the photo and talk about my journey meal prepping.\", img], stream=True)\n",
    "response.resolve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #FF6D6B\">\n",
    "\n",
    "#### Chat Completion\n",
    "\n",
    "</span>\n",
    "\n",
    "<span style=\"color: #805ec7\">\n",
    "Note: The Gemini 1.0 Pro Vision model (for text-and-image input) is not yet optimized for multi-turn conversations. Instead, use a Gemini 1.5 model or the Gemini 1.0 Pro model.\n",
    "</span>\n",
    "\n",
    "\n",
    "Gemini provides various models that can handle multimodal input (Gemini 1.5 models and Gemini 1.0 Pro Vision) so that you can input both text and images. Image requirements for prompts. [Link](https://ai.google.dev/docs/\n",
    "gemini_api_overview#image_requirements)\n",
    "\n",
    "When the prompt input includes both text and images, use a Gemini 1.5 model or the Gemini 1.0 Pro Vision model with the GenerativeModel.generate_content method to generate text output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #30AADD\">\n",
    "\n",
    "##### Without Streaming\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #FF6D6B\">\n",
    "\n",
    "#### Chat Completion\n",
    "\n",
    "</span>\n",
    "\n",
    "<span style=\"color: #805ec7\">\n",
    "Note: The Gemini 1.0 Pro Vision model (for text-and-image input) is not yet optimized for multi-turn conversations. Instead, use a Gemini 1.5 model or the Gemini 1.0 Pro model.\n",
    "</span>\n",
    "\n",
    "\n",
    "Gemini provides various models that can handle multimodal input (Gemini 1.5 models and Gemini 1.0 Pro Vision) so that you can input both text and images. Image requirements for prompts. [Link](https://ai.google.dev/docs/\n",
    "gemini_api_overview#image_requirements)\n",
    "\n",
    "When the prompt input includes both text and images, use a Gemini 1.5 model or the Gemini 1.0 Pro Vision model with the GenerativeModel.generate_content method to generate text output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "chat = model.start_chat(history=[])\n",
    "chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<google.generativeai.generative_models.ChatSession at 0x7b7b68250100>\n",
    "\n",
    "The ChatSession.send_message method returns the same GenerateContentResponse type as GenerativeModel.generate_content. It also appends your message and the response to the chat history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat.send_message(\"In one sentence, explain how a computer works to a young child.\")\n",
    "response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[parts {\n",
    "   text: \"In one sentence, explain how a computer works to a young child.\"\n",
    " }\n",
    " role: \"user\",\n",
    " parts {\n",
    "   text: \"A computer is like a very smart machine that can understand and follow our instructions, help us with our work, and even play games with us!\"\n",
    " }\n",
    " role: \"model\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #30AADD\">\n",
    "\n",
    "##### With Streaming\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat.send_message(\"Okay, how about a more detailed explanation to a high schooler?\", stream=True)\n",
    "\n",
    "for chunk in response:\n",
    "  print(chunk.text)\n",
    "  print(\"_\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in chat.history:\n",
    "  print(f'**{message.role}**: {message.parts[0].text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #FF6D6B\">\n",
    "\n",
    "#### Counting Tokens\n",
    "\n",
    "</span>\n",
    "\n",
    "<span style=\"color: #805ec7\">\n",
    "Note: The Gemini 1.0 Pro Vision model (for text-and-image input) is not yet optimized for multi-turn conversations. Instead, use a Gemini 1.5 model or the Gemini 1.0 Pro model.\n",
    "</span>\n",
    "\n",
    "\n",
    "Gemini provides various models that can handle multimodal input (Gemini 1.5 models and Gemini 1.0 Pro Vision) so that you can input both text and images. Image requirements for prompts. [Link](https://ai.google.dev/docs/\n",
    "gemini_api_overview#image_requirements)\n",
    "\n",
    "When the prompt input includes both text and images, use a Gemini 1.5 model or the Gemini 1.0 Pro Vision model with the GenerativeModel.generate_content method to generate text output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #30AADD\">\n",
    "\n",
    "##### For a string\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.count_tokens(\"What is the meaning of life?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #30AADD\">\n",
    "\n",
    "##### For a ChatSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.count_tokens(chat.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
